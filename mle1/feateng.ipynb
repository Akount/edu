{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feateng.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GAL-UUWK2rSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Feature Engineering </h1>\n",
        "\n",
        "In this notebook, you will learn how to incorporate feature engineering into your pipeline. This includes:\n",
        "<ul>\n",
        "<li> Working with feature columns </li>\n",
        "<li> Adding feature crosses in TensorFlow </li>\n",
        "<li> Reading data from BigQuery </li>\n",
        "<li> Creating datasets using Dataflow </li>\n",
        "<li> Using a wide-and-deep model </li>\n",
        "</ul>"
      ]
    },
    {
      "metadata": {
        "id": "U8ihmFF42w9H",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@markdown Copy-paste your GCP Project ID in the following field:\n",
        "\n",
        "PROJECT = \"\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "#@markdown When running this cell you will need to **uncheck \"Reset all runtimes before running\"** as shown on the following screenshot:\n",
        "#@markdown ![](https://i.imgur.com/9dgw0h0.png)\n",
        "#@markdown Next, use Shift-Enter to run this cell and to complete authentication.\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"AUTHENTICATED\")\n",
        "except:\n",
        "  print(\"FAILED to authenticate\")\n",
        "  \n",
        "REGION = \"us-central1\"   \n",
        "BUCKET = PROJECT\n",
        "\n",
        "# Copy taxi-*.csv files from github if they are missing from the runtime.\n",
        "!wget -nc --quiet https://github.com/osipov/training-data-analyst/raw/master/bootcamps/serverless_ml/taxi-11k-datasets.zip  \n",
        "!unzip -q -n taxi-11k-datasets.zip  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2hh1tIB2rSU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Environment variables for project and bucket </h2>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mAkP-6w32rSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for bash\n",
        "import os\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['TF_VERSION'] = '1.12' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vk4PlufJ2rSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U9KQwva4uHh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "rm -rf taxifare\n",
        "mkdir -p taxifare/trainer\n",
        "\n",
        "for file in taxifare/setup.py \\\n",
        "            taxifare/trainer/__init__.py \\\n",
        "            taxifare/trainer/model.py \\\n",
        "            taxifare/trainer/task.py\n",
        "do\n",
        "  wget --quiet -nc \\\n",
        "  https://github.com/osipov/edu/raw/master/mle1/feateng/$file \\\n",
        "  -O $file\n",
        "done\n",
        "\n",
        "find taxifare"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gV_JWkeT2rTK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After running the next cell, notice the new placeholders for features in the INPUT_COLUMNS. The two new categorical features are for the day of the week (`dayofweek`) and the hour of the day (`hourofday`). Also, there are three engineered feature placeholders: two for representing differences between latitude and longitude coordinates (`latdiff` and `londiff`) and one with an estimated euclidean distance (`euclidean`)."
      ]
    },
    {
      "metadata": {
        "id": "d6f3VuzNrsbV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep -m 1 -A 16 INPUT_COLUMNS taxifare/trainer/model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gvJE36DWQ2g1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next cell highlights the changes in  `build estimator` to use bucketized features and feature crosses. The NumPy `np.linspace` function divides up a range into a fixed number of partitions. In this case, each of the NYC lat/lon coordinates for a taxi ride are placed into one of  `nbucket`  buckets. Finally, both location buckets and day/hour features are feature crossed to create 4 additional features."
      ]
    },
    {
      "metadata": {
        "id": "JwVyKMEZvwcb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep -m 1 -A 22 build_estimator taxifare/trainer/model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MaFTw_W5dnx-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The feature definitions are grouped into wide and deep columns as shown in the next cell..."
      ]
    },
    {
      "metadata": {
        "id": "XG_yJx5RvYDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep -m 1 -A 20 wide_columns taxifare/trainer/model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_jLHTPynd57y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "...and the model is modified to use the wide-and-deep implementation called `DNNLinearCombinedRegressor`."
      ]
    },
    {
      "metadata": {
        "id": "2rL3wIxRwdBX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep -m 1 -A 8 DNNLinearCombinedRegressor taxifare/trainer/model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vyuhYTyXeH72",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that in the next cell, the values for the location difference and euclidean distance features are computed in-memory, as a part of the TensorFlow model implementation."
      ]
    },
    {
      "metadata": {
        "id": "jE2_9qR_wl7r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!grep -m 1 -A 14 add_engineered taxifare/trainer/model.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KSnaBLMk2rTi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Hyper-parameter tune </h2>\n",
        "\n",
        "Based on my hyperparameter tuning experiments, I ended up choosing the following values:\n",
        "<ol>\n",
        "<li> train_batch_size: 512 </li>\n",
        "<li> nbuckets: 16 </li>\n",
        "<li> hidden_units: \"64 64 64 8\" </li>    \n",
        "</ol>\n",
        "\n",
        "This gives an RMSE of **$5.7**, a considerable improvement from the 8.3 we were getting earlier ... Let's try this over a larger dataset."
      ]
    },
    {
      "metadata": {
        "id": "yAm8W0812rTj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> Run Cloud training on 2 million row dataset </h1>\n",
        "\n",
        "This run uses as input 2 million rows and takes ~70 minutes with 10 workers (STANDARD_1 pricing tier). The model is exactly the same as above. The only changes are to the input (to use the larger dataset) and to the Cloud MLE tier (to use STANDARD_1 instead of BASIC -- STANDARD_1 is approximately 10x more powerful than BASIC). \n",
        "\n",
        "When doing distributed training, use train_steps instead of num_epochs. The distributed workers don't know how many rows there are, but we can calculate train_steps = num_rows \\* num_epochs / train_batch_size. In this case, we have 2141023 * 100 / 512 = 418168 train steps."
      ]
    },
    {
      "metadata": {
        "id": "Dvy_NDk12rTj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "OUTDIR=gs://${BUCKET}/taxifare/feateng2.2m\n",
        "JOBNAME=feateng_$(date -u +%y%m%d_%H%M%S)\n",
        "TIER=STANDARD_1 \n",
        "\n",
        "echo $OUTDIR $REGION $JOBNAME\n",
        "gsutil -m rm -rf $OUTDIR\n",
        "\n",
        "gcloud ml-engine jobs submit training $JOBNAME \\\n",
        "   --region=$REGION \\\n",
        "   --module-name=trainer.task \\\n",
        "   --package-path=${PWD}/taxifare/trainer \\\n",
        "   --job-dir=$OUTDIR \\\n",
        "   --staging-bucket=gs://$BUCKET \\\n",
        "   --scale-tier=$TIER \\\n",
        "   --runtime-version=$TF_VERSION \\\n",
        "   -- \\\n",
        "   --train_data_paths=\"gs://kmo-us-central1-misc/taxifare/2.2m/1.csv*\" \\\n",
        "   --eval_data_paths=\"gs://kmo-us-central1-misc/taxifare/2.2m/2.csv*\"  \\\n",
        "   --output_dir=$OUTDIR \\\n",
        "   --train_steps=418168 \\\n",
        "   --train_batch_size=512 --nbuckets=16 --hidden_units=\"64 64 64 8\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5Kg_qBDXRJ6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After you submit the job you should see a message confirming that your job was QUEUED. To monitor the progress of the job from the GCP user interface, navigate to [Jobs](https://console.cloud.google.com/mlengine/jobs) part of the Cloud ML Engine service. Use the \"View Logs\" link to get the details."
      ]
    },
    {
      "metadata": {
        "id": "E19g6ATS2rTm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Start Tensorboard"
      ]
    },
    {
      "metadata": {
        "id": "SIkq_D2jpAcX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Instead of having you wait for the model to complete training, I have already pre-trained a model on 2.2m rows of data. Use the next cell to start TensorBoard and review the average_loss and RMSE. Note that after about 1 hr 30 min of training, the model was evaluated at roughly **$4** RMSE."
      ]
    },
    {
      "metadata": {
        "id": "vOzV5fwfpDAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorboard==1.13.0\n",
        "%reload_ext tensorboard.notebook \n",
        "%tensorboard --logdir 'gs://kmo-us-central1-misc/taxifare/feateng2.2m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sH5LW7XqoUH7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compare it to the following visualization of the model trained on 10x as much data. Notice that evaluation RMSE is roughly the same. This means that the model designed for the problem is no longer benefiting from the additional data. Nonetheless, **these models achieved and exceeded the original goal of RMSE of $6 or less!**"
      ]
    },
    {
      "metadata": {
        "id": "sz8JsfyJ2rTn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir 'gs://kmo-us-central1-misc/taxifare/feateng22m'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "noMWBnHu2rTr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Conclusions"
      ]
    },
    {
      "metadata": {
        "id": "cZCfh0Ya2rTt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The RMSE after training on the 2-million-row dataset is **$4.1**.  This graph shows the improvements you have achieved in this session."
      ]
    },
    {
      "metadata": {
        "id": "qlYAGYjd2rTu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.DataFrame({'Method' : pd.Series(['Heuristic Benchmark', 'tf.learn', '+Feature Eng.', '+ Hyperparam', '+ 2m rows']),\n",
        "              'RMSE': pd.Series([8.026, 9.4, 8.3, 5.7, 4.1]) })\n",
        "\n",
        "ax = sns.barplot(data = df, x = 'Method', y = 'RMSE')\n",
        "ax.set_ylabel('RMSE (dollars)')\n",
        "ax.set_xlabel('Methods')\n",
        "plt.plot(np.linspace(-20, 120, 1000), [5] * 1000, 'b');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bQCzitOC2rT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Counter Factual .AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    },
    {
      "metadata": {
        "id": "Z4ZSoPEV55gM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>OPTIONAL: Scalable retrieval of millions of rows of data from BigQuery</h1> "
      ]
    },
    {
      "metadata": {
        "id": "c3FTkWru2x29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install apache-beam[gcp] google-apitools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hC79ITxCYt_d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can ignore the message to restart the runtime and continue with the rest of the notebook."
      ]
    },
    {
      "metadata": {
        "id": "NIBr7yL32rSP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import apache_beam as beam\n",
        "import shutil\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0ajbcsj2rSd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Specifying query to pull the data </h2>\n",
        "\n",
        "Let's pull out a few extra columns from the timestamp."
      ]
    },
    {
      "metadata": {
        "id": "jStP19DG2rSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_query(phase, EVERY_N):\n",
        "  \"\"\"\n",
        "  phase: 1=train 2=valid\n",
        "  \"\"\"\n",
        "  base_query = \"\"\"\n",
        "    SELECT\n",
        "      (tolls_amount + fare_amount) AS fare_amount,\n",
        "      \n",
        "      CONCAT( STRING(pickup_datetime), \n",
        "              CAST(pickup_longitude AS STRING), \n",
        "              CAST(pickup_latitude AS STRING),\n",
        "              CAST(dropoff_latitude AS STRING), \n",
        "              CAST(dropoff_longitude AS STRING)) AS key,\n",
        "              \n",
        "      EXTRACT(DAYOFWEEK FROM pickup_datetime) AS dayofweek,\n",
        "      EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
        "      pickup_longitude AS pickuplon,\n",
        "      pickup_latitude AS pickuplat,\n",
        "      dropoff_longitude AS dropofflon,\n",
        "      dropoff_latitude AS dropofflat,\n",
        "      passenger_count*1.0 AS passengers\n",
        "    FROM\n",
        "      `nyc-tlc.yellow.trips`\n",
        "    WHERE\n",
        "      {}\n",
        "      AND trip_distance > 0\n",
        "      AND fare_amount >= 2.5\n",
        "      AND pickup_longitude > -78\n",
        "      AND pickup_longitude < -70\n",
        "      AND dropoff_longitude > -78\n",
        "      AND dropoff_longitude < -70\n",
        "      AND pickup_latitude > 37\n",
        "      AND pickup_latitude < 45\n",
        "      AND dropoff_latitude > 37\n",
        "      AND dropoff_latitude < 45\n",
        "      AND passenger_count > 0\n",
        "  \"\"\"\n",
        "  if EVERY_N == None:\n",
        "    if phase < 2:\n",
        "      # training\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) < 2\"\n",
        "    else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) = 2\"\n",
        "  else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), %d) = %d\" % (EVERY_N, phase)\n",
        "    \n",
        "  query = base_query.format(selector)\n",
        "\n",
        "  return query\n",
        "\n",
        "sql = create_query(2, 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9b5z9b02rSn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Preprocessing Dataflow job from BigQuery </h2>\n",
        "\n",
        "While we could read from BQ directly from TensorFlow (See: https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV.  Let's use Dataflow to do this at scale."
      ]
    },
    {
      "metadata": {
        "id": "k1EdYSQ-2rSo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gsutil -m rm -rf gs://$BUCKET/taxifare/taxi_preproc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-diHQ6p62rSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "####\n",
        "# Arguments:\n",
        "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
        "#     which each row is represented as a python dictionary\n",
        "# Returns:\n",
        "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
        "#     converted from int to string (e.g. 3 --> Tue)\n",
        "####\n",
        "def to_csv(rowdict):\n",
        "  days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
        "  CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
        "  rowdict['dayofweek'] = days[rowdict['dayofweek']]\n",
        "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
        "  return rowstring\n",
        "\n",
        "\n",
        "####\n",
        "# Arguments:\n",
        "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
        "#     Larger values will yield smaller sample\n",
        "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
        "#     locally or on Google Cloud respectively. \n",
        "# Side-effects:\n",
        "#   -Creates and executes dataflow pipeline. \n",
        "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
        "####\n",
        "def preprocess(EVERY_N, RUNNER):\n",
        "  phase_name = ['', 'train', 'valid']\n",
        "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
        "  print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
        "  OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc/'.format(BUCKET)\n",
        "\n",
        "  #dictionary of pipeline options\n",
        "  options = {\n",
        "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
        "    'project': PROJECT,\n",
        "    'runner': RUNNER\n",
        "  }\n",
        "  #instantiate PipelineOptions object using options dictionary\n",
        "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "  #instantantiate Pipeline object using PipelineOptions\n",
        "  p = beam.Pipeline(options=opts)\n",
        "  for phase in [1,2]:\n",
        "    query = create_query(phase, EVERY_N) \n",
        "    outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase_name[phase]))\n",
        "    (\n",
        "      p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n",
        "        | 'tocsv_{}'.format(phase) >> beam.Map(to_csv)\n",
        "        | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile))\n",
        "    )\n",
        "\n",
        "  p.run().wait_until_finish()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYwQrdx62rSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run pipeline locally using `DirectRunner`"
      ]
    },
    {
      "metadata": {
        "id": "4COA_TUg2rSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#50*10000 / 2.2k/  20s\n",
        "#50*1000 / 22k / 5 min\n",
        "#50*100 / 220k / >60 min\n",
        "preprocess(50*10000, 'DirectRunner') \n",
        "# preprocess(50*10000, 'DataflowRunner') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iRzhoR6m2rS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run the pipeline on cloud on a larger sample size, change the arguments to preprocess to use `DataflowRunner` and a different sample size. When running this on Cloud Dataflow, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. Note that it will take several minutes for the preprocessing job to launch."
      ]
    },
    {
      "metadata": {
        "id": "qv2zLqvr2rS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the job completes, observe the files created in Google Cloud Storage"
      ]
    },
    {
      "metadata": {
        "id": "1ze1F4Xy2rS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gsutil ls -l gs://$BUCKET/taxifare/taxi_preproc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eW7skIB2rTE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#print first 10 lines of first shard of train.csv\n",
        "gsutil cat \"gs://$BUCKET/taxifare/taxi_preproc/train.csv-00000-of-*\" | head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2z7J5fP077k0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Counter Factual .AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}