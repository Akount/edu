{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dataflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "GAL-UUWK2rSL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1> OPTIONAL: Scalable retrieval of millions of rows of data from BigQuery </h1>\n",
        "\n",
        "---\n",
        "Before you start, **make sure that you are logged in with your student account**. Otherwise you may incur Google Cloud charges for using this notebook. \n",
        "\n",
        "---"
      ]
    },
    {
      "metadata": {
        "id": "c3FTkWru2x29",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install apache-beam[gcp] google-apitools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "os2OkKYW_i3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Make sure to **restart the runtime** as instructed in the previous cell output before proceeding. \n",
        "\n",
        "**IMPORTANT:** Make sure that you **DO NOT reset the runtime**. Restarting and resetting the runtime are different operations. Reseting the runtime wipes out the file system and then restarts the runtime."
      ]
    },
    {
      "metadata": {
        "id": "U8ihmFF42w9H",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@markdown Copy-paste your GCP Project ID in the following field:\n",
        "\n",
        "PROJECT = \"\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "#@markdown When running this cell you will need to **uncheck \"Reset all runtimes before running\"** as shown on the following screenshot:\n",
        "#@markdown ![](https://i.imgur.com/9dgw0h0.png)\n",
        "#@markdown Next, use Shift-Enter to run this cell and to complete authentication.\n",
        "\n",
        "try:  \n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()  \n",
        "  print(\"AUTHENTICATED\")\n",
        "except:\n",
        "  print(\"FAILED to authenticate\")\n",
        "  \n",
        "REGION = \"us-central1\"   \n",
        "BUCKET = \"kmo-us-central1-misc\"\n",
        "\n",
        "# Copy taxi-*.csv files from github if they are missing from the runtime.\n",
        "!wget -nc --quiet https://github.com/osipov/training-data-analyst/raw/master/bootcamps/serverless_ml/taxi-11k-datasets.zip  \n",
        "!unzip -q -n taxi-11k-datasets.zip  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u2hh1tIB2rSU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2> Environment variables for project and bucket </h2>\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "mAkP-6w32rSY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# for bash\n",
        "import os\n",
        "os.environ['PROJECT'] = PROJECT\n",
        "os.environ['BUCKET'] = BUCKET\n",
        "os.environ['REGION'] = REGION\n",
        "os.environ['TF_VERSION'] = '1.12' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vk4PlufJ2rSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gcloud config set project $PROJECT\n",
        "gcloud config set compute/region $REGION"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NIBr7yL32rSP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import apache_beam as beam\n",
        "import shutil\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q0ajbcsj2rSd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Specifying query to pull the data </h2>\n",
        "\n",
        "Let's pull out a few extra columns from the timestamp."
      ]
    },
    {
      "metadata": {
        "id": "jStP19DG2rSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_query(phase, EVERY_N):\n",
        "  \"\"\"\n",
        "  phase: 1=train 2=valid\n",
        "  \"\"\"\n",
        "  base_query = \"\"\"\n",
        "    SELECT\n",
        "      (tolls_amount + fare_amount) AS fare_amount,\n",
        "      \n",
        "      CONCAT( STRING(pickup_datetime), \n",
        "              CAST(pickup_longitude AS STRING), \n",
        "              CAST(pickup_latitude AS STRING),\n",
        "              CAST(dropoff_latitude AS STRING), \n",
        "              CAST(dropoff_longitude AS STRING)) AS key,\n",
        "              \n",
        "      EXTRACT(DAYOFWEEK FROM pickup_datetime) AS dayofweek,\n",
        "      EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
        "      pickup_longitude AS pickuplon,\n",
        "      pickup_latitude AS pickuplat,\n",
        "      dropoff_longitude AS dropofflon,\n",
        "      dropoff_latitude AS dropofflat,\n",
        "      passenger_count*1.0 AS passengers\n",
        "    FROM\n",
        "      `nyc-tlc.yellow.trips`\n",
        "    WHERE\n",
        "      {}\n",
        "      AND trip_distance > 0\n",
        "      AND fare_amount >= 2.5\n",
        "      AND pickup_longitude > -78\n",
        "      AND pickup_longitude < -70\n",
        "      AND dropoff_longitude > -78\n",
        "      AND dropoff_longitude < -70\n",
        "      AND pickup_latitude > 37\n",
        "      AND pickup_latitude < 45\n",
        "      AND dropoff_latitude > 37\n",
        "      AND dropoff_latitude < 45\n",
        "      AND passenger_count > 0\n",
        "  \"\"\"\n",
        "  if EVERY_N == None:\n",
        "    if phase < 2:\n",
        "      # training\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) < 2\"\n",
        "    else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), 4) = 2\"\n",
        "  else:\n",
        "      selector = \"MOD(ABS(FARM_FINGERPRINT(STRING(pickup_datetime))), %d) = %d\" % (EVERY_N, phase)\n",
        "    \n",
        "  query = base_query.format(selector)\n",
        "\n",
        "  return query\n",
        "\n",
        "sql = create_query(2, 100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z9b5z9b02rSn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h2>Preprocessing Dataflow job from BigQuery </h2>\n",
        "\n",
        "While we could read from BQ directly from TensorFlow (See: https://www.tensorflow.org/api_docs/python/tf/contrib/cloud/BigQueryReader), it is quite convenient to export to CSV and do the training off CSV.  Let's use Dataflow to do this at scale."
      ]
    },
    {
      "metadata": {
        "id": "k1EdYSQ-2rSo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gsutil -m rm -rf gs://$BUCKET/taxifare/taxi_preproc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-diHQ6p62rSs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import datetime\n",
        "\n",
        "####\n",
        "# Arguments:\n",
        "#   -rowdict: Dictionary. The beam bigquery reader returns a PCollection in\n",
        "#     which each row is represented as a python dictionary\n",
        "# Returns:\n",
        "#   -rowstring: a comma separated string representation of the record with dayofweek\n",
        "#     converted from int to string (e.g. 3 --> Tue)\n",
        "####\n",
        "def to_csv(rowdict):\n",
        "  days = ['null', 'Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n",
        "  CSV_COLUMNS = 'fare_amount,dayofweek,hourofday,pickuplon,pickuplat,dropofflon,dropofflat,passengers,key'.split(',')\n",
        "  rowdict['dayofweek'] = days[rowdict['dayofweek']]\n",
        "  rowstring = ','.join([str(rowdict[k]) for k in CSV_COLUMNS])\n",
        "  return rowstring\n",
        "\n",
        "\n",
        "####\n",
        "# Arguments:\n",
        "#   -EVERY_N: Integer. Sample one out of every N rows from the full dataset.\n",
        "#     Larger values will yield smaller sample\n",
        "#   -RUNNER: 'DirectRunner' or 'DataflowRunner'. Specfy to run the pipeline\n",
        "#     locally or on Google Cloud respectively. \n",
        "# Side-effects:\n",
        "#   -Creates and executes dataflow pipeline. \n",
        "#     See https://beam.apache.org/documentation/programming-guide/#creating-a-pipeline\n",
        "####\n",
        "def preprocess(EVERY_N, RUNNER):\n",
        "  phase_name = ['', 'train', 'valid']\n",
        "  job_name = 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
        "  print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
        "  OUTPUT_DIR = 'gs://{0}/taxifare/taxi_preproc/'.format(BUCKET)\n",
        "\n",
        "  #dictionary of pipeline options\n",
        "  options = {\n",
        "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
        "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
        "    'job_name': 'preprocess-taxifeatures' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S'),\n",
        "    'project': PROJECT,\n",
        "    'runner': RUNNER\n",
        "  }\n",
        "  #instantiate PipelineOptions object using options dictionary\n",
        "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
        "  #instantantiate Pipeline object using PipelineOptions\n",
        "  p = beam.Pipeline(options=opts)\n",
        "  for phase in [1,2]:\n",
        "    query = create_query(phase, EVERY_N) \n",
        "    outfile = os.path.join(OUTPUT_DIR, '{}.csv'.format(phase_name[phase]))\n",
        "    (\n",
        "      p | 'read_{}'.format(phase) >> beam.io.Read(beam.io.BigQuerySource(query=query, use_standard_sql=True))\n",
        "        | 'tocsv_{}'.format(phase) >> beam.Map(to_csv)\n",
        "        | 'write_{}'.format(phase) >> beam.io.Write(beam.io.WriteToText(outfile))\n",
        "    )\n",
        "\n",
        "  p.run().wait_until_finish()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LYwQrdx62rSw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run pipeline locally using `DirectRunner`"
      ]
    },
    {
      "metadata": {
        "id": "4COA_TUg2rSx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#50*10000 / 2.2k/  20s\n",
        "#50*1000 / 22k / 5 min\n",
        "#50*100 / 220k / >60 min\n",
        "preprocess(50*10000, 'DirectRunner') \n",
        "# preprocess(50*10000, 'DataflowRunner') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iRzhoR6m2rS1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run the pipeline on cloud on a larger sample size, change the arguments to preprocess to use `DataflowRunner` and a different sample size. When running this on Cloud Dataflow, you should go to the GCP Console (https://console.cloud.google.com/dataflow) to look at the status of the job. Note that it will take several minutes for the preprocessing job to launch."
      ]
    },
    {
      "metadata": {
        "id": "qv2zLqvr2rS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once the job completes, observe the files created in Google Cloud Storage"
      ]
    },
    {
      "metadata": {
        "id": "1ze1F4Xy2rS7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "gsutil ls -l gs://$BUCKET/taxifare/taxi_preproc/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eW7skIB2rTE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "#print first 10 lines of first shard of train.csv\n",
        "gsutil cat \"gs://$BUCKET/taxifare/taxi_preproc/train.csv-00000-of-*\" | head"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2z7J5fP077k0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Copyright 2019 Counter Factual .AI LLC. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
      ]
    }
  ]
}